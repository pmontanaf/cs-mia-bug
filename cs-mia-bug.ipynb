{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug found on the paper CS-MIA\n",
    "\n",
    "### The problem\n",
    "<img src=\"figure.png\" alt=\"figure\" width=\"700\"/>\n",
    "\n",
    "Suppose we run 3 rounds of federated training, so we will have three shadow models, $M_s^1, M_s^2, M_s^3$, and three target models $M_t^1, M_t^2, M_t^3$. For simplicity, let's assume the shadow dataset consists of two training inputs $x_{\\text{in}}^1, x_{\\text{in}}^2$, and we take two other inputs not used for training, $x_{\\text{out}}^1, x_{\\text{out}}^2$. In this case, following the figure above, the training dataset for the attack model will be as follows:\n",
    "\n",
    "| Conf 1| Conf 2 | Conf 3 | Label |\n",
    "|-------------------|-------------------|-------------------|-------------------|\n",
    "| $M_s^1(x_{\\text{in}}^1)$ | $M_s^2(x_{\\text{in}}^1)$ | $M_s^3(x_{\\text{in}}^1)$ | 1 |\n",
    "| $M_s^1(x_{\\text{in}}^2)$ | $M_s^2(x_{\\text{in}}^2)$ | $M_s^3(x_{\\text{in}}^2)$ | 1 |\n",
    "| $M_s^1(x_{\\text{out}}^1)$ | $M_s^2(x_{\\text{out}}^1)$ | $M_s^3(x_{\\text{out}}^1)$ | 0 |\n",
    "| $M_s^1(x_{\\text{out}}^2)$ | $M_s^2(x_{\\text{out}}^2)$ | $M_s^3(x_{\\text{out}}^2)$ | 0 |\n",
    "\n",
    "where $M_s^i(x_{in}^j)$ is the confidence of the prediction of the model $M_s^i$ over the input $x_{in}^j$.\n",
    "\n",
    "In the code provided by the paper's authors, they build this dataset by columns. Since each column corresponds to a model, they use a loop that iterates over the models. However, in each iteration, the randomness of PyTorch's DataLoader causes the order in which they receive inputs to be different. For example, they might receive the list $(x_{\\text{in}}^1, x_{\\text{in}}^2, x_{\\text{out}}^1, x_{\\text{out}}^2)$ in the first iteration, but receive the list $(x_{\\text{in}}^2, x_{\\text{in}}^1, x_{\\text{out}}^1, x_{\\text{out}}^2)$ in the second iteration, resulting in the first two columns being in the form\n",
    "\n",
    "| Conf 1| Conf 2 |\n",
    "|-------------------|-------------------|\n",
    "| $M_s^1(x_{\\text{in}}^1)$ | $M_s^2(x_{\\text{in}}^2)$ | \n",
    "| $M_s^1(x_{\\text{in}}^2)$ | $M_s^2(x_{\\text{in}}^1)$ |\n",
    "| $M_s^1(x_{\\text{out}}^1)$ | $M_s^2(x_{\\text{out}}^1)$ | \n",
    "| $M_s^1(x_{\\text{out}}^2)$ | $M_s^2(x_{\\text{out}}^2)$ |\n",
    "\n",
    "which has no sense. The same is happening when they build the dataset for the inference phase.\n",
    "\n",
    "### Experiment\n",
    "\n",
    "> **Note**: due to the randomness of the DataLoader you may obtain different results, but there will be the same problem.\n",
    "\n",
    "For simplicity, we will take 10 rows from the MNIST dataset, and will use 5 as members and 5 as non-members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = MNIST(root = \"data/\", transform = ToTensor(), download = True)\n",
    "members_dataset = Subset(total_dataset, np.random.choice(np.arange(1, 1000), 5, replace = False))\n",
    "non_members_dataset = Subset(total_dataset, np.random.choice(np.arange(2000, 3000), 5, replace = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create 3 toy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(28 * 28, 64)\n",
    "        self.classifier = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.layer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model1 = ToyNN()\n",
    "model2 = ToyNN()\n",
    "model3 = ToyNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the functions used by the authors to create the attack training dataset, but we print the batch targets in each iteration over the DataLoader, in order to check whether we are receiving the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_conf_assurance(model, data_loader):\n",
    "    final_result = []\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, targets) in enumerate(data_loader):\n",
    "            print(f\"Batch targets: {targets.squeeze().tolist()}\")\n",
    "            predictions = model(inputs)\n",
    "            # log_softmax = nn.LogSoftmax(dim=1)\n",
    "            log_softmax = torch.nn.Softmax(dim=1)\n",
    "            predictions = log_softmax(predictions)\n",
    "            result = torch.zeros(predictions.shape[0], dtype=torch.float32)\n",
    "            for index, temp in enumerate(predictions):\n",
    "                result[index] = temp[targets[index]]\n",
    "            final_result += result.numpy().tolist()\n",
    "    return torch.Tensor(final_result)\n",
    "\n",
    "def get_dataset(models, train_loader, test_loader):\n",
    "        # decide how to compute prediction confidence\n",
    "        attack_train_x = []\n",
    "        cal_assurance_function = cal_conf_assurance\n",
    "\n",
    "        # compute confidence series of members\n",
    "        print(\"Members:\\n\")\n",
    "        i = 0\n",
    "        for model in models:\n",
    "            i += 1\n",
    "            print(\"-\"*10 + f\"Confidences for model {i}\")\n",
    "            if isinstance(model, dict):\n",
    "                model = model['model']\n",
    "            assurance = cal_assurance_function(model, train_loader)\n",
    "            attack_train_x.append(assurance.cpu().detach().numpy().tolist())\n",
    "        attack_train_x = torch.tensor(attack_train_x).t()\n",
    "        # assign confidence series of members with label 1\n",
    "        attack_train_y = torch.ones(attack_train_x.shape[0], dtype=torch.long)\n",
    "\n",
    "        # compute confidence series of non-members\n",
    "        attack_test_x = []\n",
    "        print(\"\\n\\nNon-members:\\n\")\n",
    "        i = 0\n",
    "        for model in models:\n",
    "            i += 1\n",
    "            print(\"-\"*10 + f\"Confidences for model {i}\")\n",
    "            if isinstance(model, dict):\n",
    "                model = model['model']\n",
    "            assurance = cal_assurance_function(model, test_loader)\n",
    "            attack_test_x.append(assurance.cpu().detach().numpy().tolist())\n",
    "        attack_test_x = torch.tensor(attack_test_x).t()\n",
    "        # assign confidence series of non-members with label 0\n",
    "        attack_test_y = torch.zeros(attack_test_x.shape[0], dtype=torch.long)\n",
    "\n",
    "        # combine members and non-members as dataset for attack model\n",
    "        x = torch.cat((attack_train_x, attack_test_x), 0)\n",
    "        y = torch.cat((attack_train_y, attack_test_y), 0)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Members:\n",
      "\n",
      "----------Confidences for model 1\n",
      "Batch targets: [7, 7, 3]\n",
      "Batch targets: [0, 0]\n",
      "----------Confidences for model 2\n",
      "Batch targets: [0, 7, 0]\n",
      "Batch targets: [3, 7]\n",
      "----------Confidences for model 3\n",
      "Batch targets: [7, 0, 0]\n",
      "Batch targets: [3, 7]\n",
      "\n",
      "\n",
      "Non-members:\n",
      "\n",
      "----------Confidences for model 1\n",
      "Batch targets: [8, 6, 9]\n",
      "Batch targets: [7, 3]\n",
      "----------Confidences for model 2\n",
      "Batch targets: [9, 7, 3]\n",
      "Batch targets: [8, 6]\n",
      "----------Confidences for model 3\n",
      "Batch targets: [7, 8, 9]\n",
      "Batch targets: [3, 6]\n",
      "\n",
      "\n",
      "Dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0724, 0.0974, 0.1017, 1.0000],\n",
       "        [0.0826, 0.0910, 0.1153, 1.0000],\n",
       "        [0.0965, 0.1308, 0.1168, 1.0000],\n",
       "        [0.0871, 0.0846, 0.1000, 1.0000],\n",
       "        [0.1039, 0.0959, 0.1149, 1.0000],\n",
       "        [0.1042, 0.1193, 0.1133, 0.0000],\n",
       "        [0.1077, 0.0866, 0.0936, 0.0000],\n",
       "        [0.1000, 0.0949, 0.1084, 0.0000],\n",
       "        [0.0693, 0.0825, 0.1181, 0.0000],\n",
       "        [0.1072, 0.1167, 0.0944, 0.0000]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [model1, model2, model3]\n",
    "train_loader = DataLoader(members_dataset, batch_size = 3, shuffle = True)\n",
    "test_loader = DataLoader(non_members_dataset, batch_size = 3, shuffle = True)\n",
    "attack_x, attack_y = get_dataset(models, train_loader, test_loader)\n",
    "print(\"\\n\\nDataset:\")\n",
    "torch.cat([attack_x, attack_y.view(10,1)], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the batches are not the same for each model. Let's now take one input and calculate its confidence series by hand, so that we can check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conf 1: 0.0724\n",
      "----------\n",
      "Conf 2: 0.0910\n",
      "----------\n",
      "Conf 3: 0.1149\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "input, target = members_dataset[3]\n",
    "i = 0\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "for model in models:\n",
    "    i += 1\n",
    "    pred = softmax(model(input)).squeeze().tolist()\n",
    "    print(f\"Conf {i}: {pred[target]:.4f}\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the confidences of this input are not in the same row of the dataset. In fact, conf 1 is in the first row, conf 2 is in the second row and conf 3 is in the fifth row."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
